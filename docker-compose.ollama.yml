# Ollama 専用サーバー用 (t3.large 8GB RAM 想定)
# 用途: App と別 EC2（例: 52.203.35.157）で Ollama のみ稼働。
#       .env.prod の OLLAMA_BASE_URL=http://<このサーバーのIP>:11434/v1 で App から参照。
# 同一 EC2 に載せる場合は docker-compose.prod の ollama サービスを使用。
#
# 起動: docker compose -f docker-compose.ollama.yml up -d
# モデル取得: docker exec ollama ollama pull qwen2.5:3b
# 疎通確認: curl -s http://localhost:11434/api/tags

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: always
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_GPU_MEMORY=0
      # コンテキスト: 4096=会話の長さ。8GB なら 2–3B モデルで余裕
      - OLLAMA_NUM_CTX=4096
      # モデルを RAM に保持 (再ロード防止)
      - OLLAMA_KEEP_ALIVE=60m
      # 同時リクエスト数 (2-3B なら 2 が安全)
      - OLLAMA_NUM_PARALLEL=2
      # 1 モデルのみロード (メモリ節約)
      - OLLAMA_MAX_LOADED_MODELS=1
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 7G
        reservations:
          cpus: '1'
          memory: 2G
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 90s

volumes:
  ollama_data:
